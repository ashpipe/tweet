{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ALICT2faXgsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fbb44d-baa1-4798-8f3d-70f6cd4ed7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from utils import Layer, load_tweets#, process_tweet\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split():\n",
        "    # Load positive and negative tweets\n",
        "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "\n",
        "    # View the total number of positive and negative tweets.\n",
        "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
        "\n",
        "    # Split positive set into validation and training\n",
        "    val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
        "    train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
        "\n",
        "    # Split negative set into validation and training\n",
        "    val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
        "    train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
        "    \n",
        "    # Combine training data into one set\n",
        "    train_x = train_pos + train_neg \n",
        "\n",
        "    # Combine validation data into one set\n",
        "    val_x  = val_pos + val_neg\n",
        "\n",
        "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
        "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
        "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "\n",
        "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y\n",
        "\n",
        "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFQrWG04X298",
        "outputId": "82ed6898-f6b2-4b58-83e1-2da7bbd48912"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n",
            "length of train_x 8000\n",
            "length of val_x 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "OQcGh3q0c25H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "        #if word not in string.punctuation:\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = word #stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n",
        "process_tweet(train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80HVM_jHedn6",
        "outputId": "ee27c530-f1c2-4ede-87c4-1b0b8953a7d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out function that processes tweets\n",
        "print(\"original tweet at training position 0\")\n",
        "print(train_pos[0])\n",
        "\n",
        "print(\"Tweet at training position 0 after processing:\")\n",
        "process_tweet(train_pos[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q-XE67BX_vI",
        "outputId": "3bf09b18-53d8-48f2-f57f-bdb030277901"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original tweet at training position 0\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "Tweet at training position 0 after processing:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m,)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]                                   # number of training examples\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
        "    X_indices = np.zeros((m,max_len))\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        #sentence_words = X[i].lower().split()\n",
        "        sentence_words = process_tweet(X[i].lower())\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "\n",
        "        for w in sentence_words:\n",
        "            # if w exists in the word_to_index dictionary\n",
        "            if w in word_to_index:\n",
        "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "                X_indices[i, j] = word_to_index[w]\n",
        "                # Increment j to j + 1\n",
        "                j =  j+1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_indices\n",
        "\n",
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\", train_x[0]],)\n",
        "X1_indices = sentences_to_indices(X1, word_to_index, max_len=10)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\\n\", X1_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OaSLjRgc3Tt",
        "outputId": "2f95e7aa-f79d-48a5-ee14-72b6dfe7dcfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you'\n",
            " '#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)']\n",
            "X1_indices =\n",
            " [[155345. 225122.      0.      0.      0.      0.      0.      0.      0.\n",
            "       0.]\n",
            " [220930. 286375.  69714.      0.      0.      0.      0.      0.      0.\n",
            "       0.]\n",
            " [151204. 302254.      0.      0.      0.      0.      0.      0.      0.\n",
            "       0.]\n",
            " [362050. 137337. 240688. 106074. 384714.  42869.      0.      0.      0.\n",
            "       0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_size = len(word_to_index) + 1              # adding 1 to fit Keras embedding (requirement)\n",
        "    any_word = list(word_to_vec_map.keys())[0]\n",
        "    emb_dim = word_to_vec_map[any_word].shape[0]    # define dimensionality of your GloVe word vectors (= 50)\n",
        "      \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1\n",
        "    # Initialize the embedding matrix as a numpy array of zeros.\n",
        "    # See instructions above to choose the correct shape.\n",
        "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
        "    \n",
        "    # Step 2\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        try:\n",
        "            emb_matrix[idx, :] = word_to_vec_map[word]\n",
        "        except:\n",
        "            # \"revealed\" has only 40 dimension\n",
        "            emb_matrix[idx, :] = np.append(word_to_vec_map[word], np.zeros(10))\n",
        "\n",
        "    # Step 3\n",
        "    # Define Keras embedding layer with the correct input and output sizes\n",
        "    # Make it non-trainable.\n",
        "    embedding_layer = Embedding(vocab_size, emb_dim, trainable=False)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Step 4 (already done for you; please do not modify)\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
        "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "metadata": {
        "id": "ysBVb0ZTnd8c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analyze(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Function creating the sentiment analyze model's graph.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Define sentence_indices as the input of the graph.\n",
        "    # It should be of shape input_shape and dtype 'int32' (as it contains indices, which are integers).\n",
        "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # Propagate sentence_indices through your embedding layer\n",
        "    # (See additional hints in the instructions).\n",
        "    embeddings = embedding_layer(sentence_indices)   \n",
        "    \n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a batch of sequences.\n",
        "    X = LSTM(units = 128, return_sequences = True)(embeddings)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(rate=0.5)(X)\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = LSTM(units=64, return_sequences = False)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(rate=0.5)(X)\n",
        "    # Propagate X through a Dense layer with 5 units\n",
        "    X = Dense(units=1)(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('sigmoid')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(inputs=sentence_indices, outputs= X)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Ou867jJ5oERE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxLen = len(max(train_x, key=len).split())\n",
        "model = sentiment_analyze((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA-bBcObopE9",
        "outputId": "d0baa4d7-7970-43f0-9581-fa14c32ac93b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28)]              0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 28, 50)            20000050  \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 28, 128)           91648     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 28, 128)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,141,171\n",
            "Trainable params: 141,121\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PPWt13ZzrvMX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_indices = sentences_to_indices(np.array(train_x + val_x), word_to_index, maxLen)\n",
        "X_train_indices[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuajDZzyrxnu",
        "outputId": "b67d757a-5868-4d0f-dd96-61b3b81821d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([362050., 137337., 240688., 106074., 384714.,  42869.,      0.,\n",
              "            0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "            0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "            0.,      0.,      0.,      0.,      0.,      0.,      0.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_indices, np.append(train_y, val_y), validation_split=0.2, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "metadata": {
        "id": "ethmnc9ar1re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe73e016-5079-46c3-8021-2ba3f66fa641"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 21s 68ms/step - loss: 0.1412 - accuracy: 0.9461 - val_loss: 0.0566 - val_accuracy: 0.9830\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0416 - accuracy: 0.9865 - val_loss: 0.0398 - val_accuracy: 0.9870\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 17s 68ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.0314 - val_accuracy: 0.9935\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0239 - accuracy: 0.9944 - val_loss: 0.0262 - val_accuracy: 0.9890\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0207 - accuracy: 0.9950 - val_loss: 0.0187 - val_accuracy: 0.9945\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.0200 - val_accuracy: 0.9940\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 0.0381 - val_accuracy: 0.9925\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0321 - accuracy: 0.9925 - val_loss: 0.0293 - val_accuracy: 0.9930\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0269 - accuracy: 0.9946 - val_loss: 0.0160 - val_accuracy: 0.9960\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0186 - accuracy: 0.9961 - val_loss: 0.0287 - val_accuracy: 0.9935\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0225 - accuracy: 0.9942 - val_loss: 0.0282 - val_accuracy: 0.9945\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0184 - accuracy: 0.9959 - val_loss: 0.0228 - val_accuracy: 0.9950\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0172 - accuracy: 0.9961 - val_loss: 0.0274 - val_accuracy: 0.9945\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 17s 68ms/step - loss: 0.0178 - accuracy: 0.9964 - val_loss: 0.0435 - val_accuracy: 0.9910\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0185 - accuracy: 0.9956 - val_loss: 0.0199 - val_accuracy: 0.9950\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0168 - accuracy: 0.9962 - val_loss: 0.0237 - val_accuracy: 0.9950\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0171 - accuracy: 0.9964 - val_loss: 0.0291 - val_accuracy: 0.9945\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0165 - accuracy: 0.9964 - val_loss: 0.0191 - val_accuracy: 0.9940\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0168 - accuracy: 0.9965 - val_loss: 0.0304 - val_accuracy: 0.9945\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0154 - accuracy: 0.9965 - val_loss: 0.0305 - val_accuracy: 0.9940\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.0317 - val_accuracy: 0.9940\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 0.0232 - val_accuracy: 0.9945\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0166 - accuracy: 0.9964 - val_loss: 0.0283 - val_accuracy: 0.9930\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0163 - accuracy: 0.9962 - val_loss: 0.0160 - val_accuracy: 0.9955\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 18s 72ms/step - loss: 0.0147 - accuracy: 0.9970 - val_loss: 0.0230 - val_accuracy: 0.9950\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0156 - accuracy: 0.9965 - val_loss: 0.0135 - val_accuracy: 0.9965\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.0280 - accuracy: 0.9941 - val_loss: 0.0180 - val_accuracy: 0.9960\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0169 - accuracy: 0.9964 - val_loss: 0.0402 - val_accuracy: 0.9930\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0169 - accuracy: 0.9959 - val_loss: 0.0237 - val_accuracy: 0.9955\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0144 - accuracy: 0.9971 - val_loss: 0.0246 - val_accuracy: 0.9945\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0135 - accuracy: 0.9974 - val_loss: 0.0318 - val_accuracy: 0.9940\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0162 - accuracy: 0.9965 - val_loss: 0.0169 - val_accuracy: 0.9955\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 0.0157 - val_accuracy: 0.9950\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.0130 - accuracy: 0.9973 - val_loss: 0.0219 - val_accuracy: 0.9945\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.0279 - val_accuracy: 0.9940\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 0.0135 - accuracy: 0.9967 - val_loss: 0.0145 - val_accuracy: 0.9960\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.0160 - val_accuracy: 0.9960\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0107 - accuracy: 0.9981 - val_loss: 0.0197 - val_accuracy: 0.9950\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9945\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.0183 - val_accuracy: 0.9960\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0108 - accuracy: 0.9981 - val_loss: 0.0165 - val_accuracy: 0.9960\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.0268 - val_accuracy: 0.9950\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.0190 - val_accuracy: 0.9955\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0139 - accuracy: 0.9973 - val_loss: 0.0273 - val_accuracy: 0.9935\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0200 - accuracy: 0.9955 - val_loss: 0.0167 - val_accuracy: 0.9945\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0157 - accuracy: 0.9965 - val_loss: 0.0140 - val_accuracy: 0.9945\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.0231 - val_accuracy: 0.9935\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0095 - accuracy: 0.9981 - val_loss: 0.0260 - val_accuracy: 0.9935\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.0228 - val_accuracy: 0.9955\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.0152 - accuracy: 0.9964 - val_loss: 0.0150 - val_accuracy: 0.9965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_indices = sentences_to_indices(np.array(val_x), word_to_index, max_len = maxLen)\n",
        "loss, acc = model.evaluate(X_test_indices, val_y)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "metadata": {
        "id": "NpktSqdzsAi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41c8e8f-f927-4441-c8ba-971628d5df8d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 1s 23ms/step - loss: 0.0150 - accuracy: 0.9965\n",
            "\n",
            "Test accuracy =  0.9965000152587891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='val')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "xklSmjDRxZ-O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d232ce0c-d425-4820-8c63-39fa111a628b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9daa802b20>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}